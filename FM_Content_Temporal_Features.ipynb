{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e862fc52",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import torch as torch\n",
    "import torch.utils.data as data\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import time\n",
    "import numpy as np\n",
    "from sklearn.manifold import TSNE\n",
    "\n",
    "import math\n",
    "from torch.utils.data import Dataset\n",
    "import itertools\n",
    "import torch.nn.functional as F\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "import gzip\n",
    "import numpy as np\n",
    "import random\n",
    "import scipy\n",
    "from collections import defaultdict\n",
    "from scipy.spatial import distance\n",
    "import dateutil\n",
    "import os\n",
    "from sklearn.metrics import roc_auc_score\n",
    "import json, time\n",
    "import pandas as pd\n",
    "def parseData(fname):\n",
    "    for l in open(fname):\n",
    "        yield eval(l)\n",
    "        \n",
    "def readDataFull(path):\n",
    "    data = []\n",
    "    for line in gzip.open(path):\n",
    "        d = eval(line)\n",
    "        data.append(d)  \n",
    "    return data\n",
    "\n",
    "def readData_full(path):\n",
    "    data = []\n",
    "    for line in open(path):\n",
    "        d = json.loads(line)\n",
    "        data.append(d)  \n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9f0daaa3",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataFolder = 'data/ca_final/'\n",
    "users_file_name = dataFolder + 'ca_users.json'\n",
    "places_file_name = dataFolder + 'ca_places.json'\n",
    "reviews_file_name = dataFolder + 'ca_final_reviews.json'\n",
    "\n",
    "data_user  = pd.DataFrame(readData_full(users_file_name))\n",
    "data_places  = pd.DataFrame(readData_full(places_file_name))\n",
    "data_reviews  = pd.DataFrame(readData_full(reviews_file_name))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f2943e7",
   "metadata": {},
   "source": [
    "### Construct data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5800dbb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "st0Min, st0Max = 32.534, 42.009\n",
    "st1Min, st1Max = -124.4, -114.13\n",
    "bk = 200\n",
    "bin0 = np.linspace(st0Min, st0Max, bk)\n",
    "bin0 = [(a, b) for a, b in zip(bin0[:-1], bin0[1:])]\n",
    "bin1 = np.linspace(st1Min, st1Max, bk)\n",
    "bin1 = [(a, b) for a, b in zip(bin1[:-1], bin1[1:])]\n",
    "\n",
    "def getBin(gps0, gps1):\n",
    "    b0, b1 = 0, 0\n",
    "    for i, (a, b) in enumerate(bin0):\n",
    "        if gps0 >= a and gps0 < b:\n",
    "            b0 = i\n",
    "    for i, (a, b) in enumerate(bin1):\n",
    "        if gps1 >= a and gps1 < b:\n",
    "            b1 = i\n",
    "    return bk * b0 + b1\n",
    "\n",
    "placesDict = {}\n",
    "for i, p in data_places.iterrows():\n",
    "    placesDict[p['gPlusPlaceId']] = p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "28ad4ece",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "places_price_dict = {}\n",
    "places_hours_dict = {}\n",
    "places_cat_dict = {}\n",
    "\n",
    "user_education = {}\n",
    "user_jobs = {}\n",
    "review_years = {}\n",
    "\n",
    "cnt = 0\n",
    "\n",
    "_uni = ['university','college', 'school','institute', 'institution','academy','polytechnic','varsity', 'academy']\n",
    "def checkeducation(ed):\n",
    "    ed = ed.lower()\n",
    "    for x in _uni:\n",
    "        if x in ed:\n",
    "            return 1\n",
    "    return 0\n",
    "\n",
    "for _i, d in data_user.iterrows():\n",
    "    ed = str(d['education'])\n",
    "    user_education[d['gPlusUserId']] = checkeducation(ed)\n",
    "    if d['jobs'] is not None:\n",
    "        user_jobs[d['gPlusUserId']] = 1\n",
    "    else:\n",
    "        user_jobs[d['gPlusUserId']] = 0\n",
    "    \n",
    "for _i, d in data_places.iterrows():\n",
    "    places_price_dict[d['gPlusPlaceId']] = str(d['price'])\n",
    "    s = str(d['hours'])\n",
    "    s = re.sub(r'[^\\w\\s]','',s)\n",
    "    s = ' '.join( [w for w in s.split() if len(w)>1] )\n",
    "    places_hours_dict[d['gPlusPlaceId']] = s\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5a144bb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "places_cat_dict['dummy'] = 'dummy'\n",
    "places_price_dict['dummy'] = 'dummy'\n",
    "places_hours_dict['dummy'] = 'dummy'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f6dfc747",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "50000\n",
      "100000\n",
      "150000\n",
      "200000\n",
      "250000\n",
      "17.33063829787234\n"
     ]
    }
   ],
   "source": [
    "userIDs = {}\n",
    "itemIDs = {}\n",
    "categoryIDs = {}\n",
    "yearIDs = {}\n",
    "priceIDs = {}\n",
    "hourIDs = {}\n",
    "educationIDs = {}\n",
    "jobIDs = {}\n",
    "\n",
    "\n",
    "interactionsPerUser = defaultdict(list)\n",
    "uniquePlaces = set()\n",
    "\n",
    "\n",
    "cnt = 0\n",
    "for _i, d in data_reviews.iterrows():\n",
    "    if cnt % 50000 == 0:\n",
    "        print(cnt)\n",
    "    cnt += 1\n",
    "    u = d['gPlusUserId']\n",
    "    i = d['gPlusPlaceId']\n",
    "    t = d['unixReviewTime']\n",
    "    r = d['rating']\n",
    "    \n",
    "    _cat = str(d['categories'])\n",
    "    \n",
    "    places_cat_dict[d['gPlusPlaceId']] = _cat\n",
    "    \n",
    "    gps = placesDict[d['gPlusPlaceId']]['gps']\n",
    "    b = getBin(gps[0], gps[1])\n",
    "    \n",
    "    _year = 'dummy'\n",
    "    if d['reviewTime'] is not None:\n",
    "        _year = dateutil.parser.parse(d['reviewTime']).year\n",
    "    \n",
    "    _price = places_price_dict[i]\n",
    "    _hours = places_hours_dict[i]\n",
    "    _ed = 'dummy'\n",
    "    if u in user_education:\n",
    "        _ed = user_education[u]\n",
    "    _job = 'dummy'\n",
    "    if u in user_jobs:\n",
    "        _job = user_jobs[u]\n",
    "    \n",
    "    uniquePlaces.add(i)\n",
    "    \n",
    "    if not u in userIDs: userIDs[u] = len(userIDs)\n",
    "    if not i in itemIDs: itemIDs[i] = len(itemIDs)\n",
    "        \n",
    "    if not _cat in categoryIDs: categoryIDs[_cat] = len(categoryIDs)\n",
    "    if not _year in yearIDs: yearIDs[_year] = len(yearIDs)\n",
    "    if not _price in priceIDs: priceIDs[_price] = len(priceIDs)\n",
    "    if not _hours in hourIDs: hourIDs[_hours] = len(hourIDs)\n",
    "    if not _ed in educationIDs: educationIDs[_ed] = len(educationIDs)\n",
    "    if not _job in jobIDs: jobIDs[_job] = len(jobIDs)\n",
    "    interactionsPerUser[u].append((t,i,r,_cat,_year,_price,_hours,_ed,_job,b))\n",
    "    \n",
    "userInteractionAvg = sum ([len(interactionsPerUser[u]) for u in interactionsPerUser])  / len(interactionsPerUser)\n",
    "print(userInteractionAvg)\n",
    "\n",
    "itemIDs['dummy'] = len(itemIDs)\n",
    "categoryIDs['dummy'] = len(categoryIDs)\n",
    "yearIDs['dummy'] = len(yearIDs)\n",
    "priceIDs['dummy'] = len(priceIDs)\n",
    "hourIDs['dummy'] = len(hourIDs)\n",
    "educationIDs['dummy'] = len(educationIDs)\n",
    "jobIDs['dummy'] = len(jobIDs)\n",
    "\n",
    "interactionstrain = []\n",
    "interactionstest = []\n",
    "for u in interactionsPerUser:\n",
    "    interactionsPerUser[u].sort()\n",
    "    list_users = interactionsPerUser[u]\n",
    "    lastItem = 'dummy'\n",
    "    prev_year = 'dummy'\n",
    "    lastB = 0\n",
    "    for (t,i,r, _cat,_year,_price,_hours,_ed,_job,b) in list_users[:-1]:\n",
    "        interactionstrain.append((u,i,lastItem, _cat,_year,_price,_hours,_ed,_job, prev_year,b, lastB))\n",
    "        lastItem = i\n",
    "        prev_year = _year\n",
    "        lastB = b\n",
    "\n",
    "    (t,i,r, _cat,_year,_price,_hours,_ed,_job,b) = list_users[-1]\n",
    "    interactionstest.append((u,i,lastItem,_cat,_year,_price,_hours,_ed,_job, prev_year, b, lastB))\n",
    "    lastItem = i\n",
    "    prev_year = _year\n",
    "    lastB = b\n",
    "\n",
    "itemsPerUser = defaultdict(set)\n",
    "for (u,i,lastItem,_cat,_year,_price,_hours,_ed,_job, prev_year, b, lastB) in interactionstrain:\n",
    "    itemsPerUser[u].add(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "cede231c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16450 62003 15255 18 6 17606 3 3\n",
      "268639\n",
      "16450\n"
     ]
    }
   ],
   "source": [
    "nUsers,nItems,nCat,nYear, nPrice, nHour, nED, nJob = len(userIDs),len(itemIDs),len(categoryIDs), \\\n",
    "                                                        len(yearIDs), len(priceIDs), len(hourIDs), \\\n",
    "                                                            len(educationIDs), len(jobIDs)\n",
    "items = list(itemIDs.keys())\n",
    "print(nUsers,nItems,nCat,nYear, nPrice, nHour, nED, nJob)\n",
    "print(len(interactionstrain))\n",
    "print(len(interactionstest))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "feb9aa1a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "173329\n",
      "[16450, 62003, 62003, 15255, 6, 17606, 3, 3]\n"
     ]
    }
   ],
   "source": [
    "#features_list = ['userId_index', 'placeId_index', 'lastplaceId_index', 'cat']\n",
    "\n",
    "# temporal features - no use \n",
    "#'year': nYear,     # no use of temporal features \n",
    "#'prev_year': nYear\n",
    "features_sizes = {\n",
    "    'userId_index': nUsers,\n",
    "    'placeId_index':nItems,\n",
    "    'lastplaceId_index':nItems,\n",
    "     'cat':nCat,\n",
    "     'price': nPrice,\n",
    "     'hour': nHour,\n",
    "      'ed': nED,\n",
    "      'job': nJob\n",
    "#     'bin': bk * bk,\n",
    "#       'prev_bin': bk * bk\n",
    "}\n",
    "\n",
    "next_offset = 0\n",
    "features_offsets={}\n",
    "index = 0\n",
    "field_dimensions = []\n",
    "for k,v in features_sizes.items():\n",
    "    features_offsets[index] = next_offset\n",
    "    index += 1\n",
    "    next_offset += v\n",
    "    field_dimensions.append(v)\n",
    "\n",
    "print(next_offset)\n",
    "print(field_dimensions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "8f9dd6ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = []\n",
    "x_train_neg = []\n",
    "\n",
    "\n",
    "def feat(x):\n",
    "    return [x[i] + features_offsets[i] for i in range(len(x))]\n",
    "\n",
    "for (u,i,j,_cat,_year,_price,_hours,_ed,_job, prev_year,b,pb) in interactionstrain:\n",
    "    uindex = userIDs[u]\n",
    "    iindex = itemIDs[i]\n",
    "    jindex = itemIDs[j]\n",
    "    \n",
    "    # temporal\n",
    "    year_index = yearIDs[_year]\n",
    "    prev_year_index = yearIDs[prev_year] \n",
    "    \n",
    "    #item based\n",
    "    cat_index = categoryIDs[_cat]\n",
    "    price_index = priceIDs[_price]\n",
    "    hour_index = hourIDs[_hours]\n",
    "    \n",
    "    #user based\n",
    "    ed_index = educationIDs[_ed]\n",
    "    job_index = jobIDs[_job]\n",
    "    \n",
    "    # overall - don't change\n",
    "    #x_train.append(feat((uindex, iindex, jindex, cat_index,price_index,hour_index,ed_index,job_index,b,pb)))\n",
    "\n",
    "    x_train.append(feat((uindex, iindex, jindex,cat_index,price_index,hour_index,ed_index,job_index)))\n",
    "    \n",
    "    k = random.choice(items) # negative sample\n",
    "    while k in itemsPerUser[u]:\n",
    "        k = random.choice(items)\n",
    "    uindex = userIDs[u]\n",
    "    kindex = itemIDs[k]\n",
    "    jindex = itemIDs[j] \n",
    "    \n",
    "    \n",
    "    # recompute for negative items \n",
    "    neg_cat = places_cat_dict[k]\n",
    "    neg_price = places_price_dict[k]\n",
    "    neg_hours = places_hours_dict[k]\n",
    "        \n",
    "    cat_index = categoryIDs[neg_cat]\n",
    "    price_index = priceIDs[neg_price]\n",
    "    hour_index = hourIDs[neg_hours]\n",
    "    \n",
    "    #users won't change\n",
    "    ed_index = educationIDs[_ed]\n",
    "    job_index = jobIDs[_job]\n",
    "    \n",
    "    #temporal won't change\n",
    "    year_index = yearIDs[_year]\n",
    "    prev_year_index = yearIDs[prev_year]\n",
    "    \n",
    "    gps = (st0Min, st1Min) if k == 'dummy' else placesDict[k]['gps']\n",
    "    negB = getBin(gps[0], gps[1])\n",
    "    \n",
    "    #overall - don't change\n",
    "    #x_train_neg.append(feat((uindex, kindex, jindex, cat_index,price_index,hour_index,ed_index,job_index,negB,pb)))\n",
    "    x_train_neg.append(feat((uindex, kindex, jindex,cat_index,price_index,hour_index,ed_index,job_index)))\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "2594763e",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_x = torch.tensor(x_train)\n",
    "data_x_neg = torch.tensor(x_train_neg)\n",
    "dataset = data.TensorDataset(data_x,data_x_neg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "677b7884",
   "metadata": {},
   "outputs": [],
   "source": [
    "bs=50000\n",
    "train_n = int(len(dataset)*0.85)\n",
    "valid_n = len(dataset) - train_n\n",
    "splits = [train_n,valid_n]\n",
    "assert sum(splits) == len(dataset)\n",
    "trainset,devset = torch.utils.data.random_split(dataset,splits)\n",
    "train_dataloader = data.DataLoader(trainset,batch_size=bs,shuffle=True)\n",
    "dev_dataloader = data.DataLoader(devset,batch_size=bs,shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "53dd3390",
   "metadata": {},
   "outputs": [],
   "source": [
    "def trunc_normal_(x, mean=0., std=1.):\n",
    "    \"Truncated normal initialization.\"\n",
    "    return x.normal_().fmod_(2).mul_(std).add_(mean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "a2f7d989",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FMModel(nn.Module):\n",
    "    def __init__(self, n, k):\n",
    "        super().__init__()\n",
    "\n",
    "        self.w0 = nn.Parameter(torch.zeros(1))\n",
    "        self.bias = nn.Embedding(n, 1)\n",
    "        self.embeddings = nn.Embedding(n, k)\n",
    "\n",
    "        with torch.no_grad(): trunc_normal_(self.embeddings.weight, std=0.01)\n",
    "        with torch.no_grad(): trunc_normal_(self.bias.weight, std=0.01)\n",
    "\n",
    "    def forward(self, X_pos, X_neg):\n",
    "        emb = self.embeddings(X_pos)\n",
    "        pow_of_sum = emb.sum(dim=1).pow(2)\n",
    "        sum_of_pow = emb.pow(2).sum(dim=1)\n",
    "        pairwise = (pow_of_sum-sum_of_pow).sum(1)*0.5\n",
    "        bias = self.bias(X_pos).squeeze().sum(1)\n",
    "        \n",
    "        pos = self.w0 + bias + pairwise\n",
    "        \n",
    "        emb = self.embeddings(X_neg)\n",
    "        pow_of_sum = emb.sum(dim=1).pow(2)\n",
    "        sum_of_pow = emb.pow(2).sum(dim=1)\n",
    "        pairwise = (pow_of_sum-sum_of_pow).sum(1)*0.5\n",
    "        bias = self.bias(X_neg).squeeze().sum(1)        \n",
    "        \n",
    "        neg = self.w0 + bias + pairwise\n",
    "        loss = -torch.mean(torch.log(torch.sigmoid(pos - neg)))\n",
    "        return loss\n",
    "    \n",
    "    def predict_1(self, X):\n",
    "        \n",
    "        emb = self.embeddings(X)\n",
    "        pow_of_sum = emb.sum(dim=1).pow(2)\n",
    "        sum_of_pow = emb.pow(2).sum(dim=1)\n",
    "        pairwise = (pow_of_sum-sum_of_pow).sum(1)*0.5\n",
    "        bias = self.bias(X).sum(1)\n",
    "        \n",
    "        return self.w0 + bias + pairwise \n",
    "    def predict_2(self, X):\n",
    "        \n",
    "        emb = self.embeddings(X)\n",
    "        pow_of_sum = emb.sum(dim=1).pow(2)\n",
    "        sum_of_pow = emb.pow(2).sum(dim=1)\n",
    "        pairwise = (pow_of_sum-sum_of_pow).sum(1)*0.5\n",
    "        bias = self.bias(X).squeeze().sum(1) \n",
    "        \n",
    "        return self.w0 + bias + pairwise "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "7561c1b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit(iterator, model, optimizer, criterion):\n",
    "    train_loss = 0\n",
    "    model.train()\n",
    "    for x_pos,x_neg in iterator:\n",
    "        optimizer.zero_grad()\n",
    "        loss = model(x_pos, x_neg)\n",
    "        train_loss += loss.item()*x_pos.shape[0]\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    return train_loss / len(iterator.dataset)\n",
    "\n",
    "def test(iterator, model, criterion):\n",
    "    train_loss = 0\n",
    "    model.eval()\n",
    "    for x_pos,x_neg in iterator:                    \n",
    "        with torch.no_grad():\n",
    "            loss = model(x_pos, x_neg)\n",
    "        train_loss += loss.item()*x_pos.shape[0]\n",
    "    return train_loss / len(iterator.dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "2813fd17",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0. time: 1[s]\n",
      "\ttrain loss: 0.6030\n",
      "\tvalidation loss: 0.5042\n",
      "epoch 1. time: 2[s]\n",
      "\ttrain loss: 0.3920\n",
      "\tvalidation loss: 0.4834\n",
      "epoch 2. time: 5[s]\n",
      "\ttrain loss: 0.2540\n",
      "\tvalidation loss: 0.4812\n",
      "epoch 3. time: 1[s]\n",
      "\ttrain loss: 0.1638\n",
      "\tvalidation loss: 0.5151\n",
      "epoch 4. time: 3[s]\n",
      "\ttrain loss: 0.1085\n",
      "\tvalidation loss: 0.5431\n",
      "epoch 5. time: 1[s]\n",
      "\ttrain loss: 0.0806\n",
      "\tvalidation loss: 0.5691\n",
      "epoch 6. time: 2[s]\n",
      "\ttrain loss: 0.0703\n",
      "\tvalidation loss: 0.5956\n",
      "epoch 7. time: 2[s]\n",
      "\ttrain loss: 0.0661\n",
      "\tvalidation loss: 0.6182\n",
      "epoch 8. time: 1[s]\n",
      "\ttrain loss: 0.0635\n",
      "\tvalidation loss: 0.6379\n",
      "epoch 9. time: 2[s]\n",
      "\ttrain loss: 0.0610\n",
      "\tvalidation loss: 0.6570\n"
     ]
    }
   ],
   "source": [
    "#model = FMModel(max(data_x_neg.max(), data_x.max())+1, 10)\n",
    "model = FMModel(next_offset + 1, 10)\n",
    "wd= 1e-5\n",
    "lr= 0.05\n",
    "epochs=5\n",
    "optimizer = optim.Adam(model.parameters(), lr=lr, weight_decay=wd)\n",
    "#scheduler = optim.lr_scheduler.MultiStepLR(optimizer, milestones=[3], gamma=0.1)\n",
    "criterion = nn.MSELoss()\n",
    "for epoch in range(epochs):\n",
    "    start_time = time.time()\n",
    "    train_loss = fit(train_dataloader, model, optimizer, criterion)\n",
    "    valid_loss = test(dev_dataloader, model, criterion)\n",
    "    #scheduler.step()\n",
    "    secs = int(time.time() - start_time)\n",
    "    print(f'epoch {epoch}. time: {secs}[s]')\n",
    "    print(f'\\ttrain loss: {((train_loss)):.4f}')\n",
    "    print(f'\\tvalidation loss: {((valid_loss)):.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36a462b7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "5000\n",
      "10000\n"
     ]
    }
   ],
   "source": [
    "interactionsTestPerUser = defaultdict(set)\n",
    "itemSet = set()\n",
    "for (u,i,j,_cat,_year,_price,_hours,_ed,_job, prev_year,b,pb ) in interactionstest:\n",
    "    interactionsTestPerUser[u].add((i,j,_cat,_year,_price,_hours,_ed,_job, prev_year,b,pb ))\n",
    "        \n",
    "    itemSet.add(i)\n",
    "    itemSet.add(j)\n",
    "    \n",
    "\n",
    "def AUCu(model, u, N):\n",
    "    win = 0\n",
    "    positive = [random.sample(interactionsTestPerUser[u],1)[0]] * N\n",
    "    negative = random.sample(itemSet,N)\n",
    "    for (i,j,cat,_year,_price,_hours,_ed,_job, prev_year,b,pb),k  in zip(positive,negative):\n",
    "        \n",
    "        #Overall -  don't change\n",
    "        #pos1 = np.array([feat((userIDs[u], itemIDs[i], itemIDs[j], categoryIDs[cat],priceIDs[_price],hourIDs[_hours], educationIDs[_ed], jobIDs[_job],b,pb))])\n",
    "        \n",
    "        pos1 = np.array([feat((userIDs[u], itemIDs[i], itemIDs[j], categoryIDs[cat],priceIDs[_price],hourIDs[_hours], educationIDs[_ed], jobIDs[_job]))])\n",
    "\n",
    "        \n",
    "        # negative \n",
    "        gps = placesDict[k]['gps']\n",
    "        negB = getBin(gps[0], gps[1])\n",
    "        \n",
    "        neg_cat = places_cat_dict[k]\n",
    "        neg_price = places_price_dict[k]\n",
    "        neg_hours = places_hours_dict[k]\n",
    "        \n",
    "        cat_index = categoryIDs[neg_cat]\n",
    "        price_index = priceIDs[neg_price]\n",
    "        hour_index = hourIDs[neg_hours]\n",
    "            \n",
    "        # Overall - don't change\n",
    "        #neg1 = np.array([feat((userIDs[u], itemIDs[k], itemIDs[j], cat_index, price_index, hour_index,educationIDs[_ed], jobIDs[_job],negB, pb ))])\n",
    "        \n",
    "        neg1 = np.array([feat((userIDs[u], itemIDs[k], itemIDs[j], cat_index, price_index, hour_index,educationIDs[_ed], jobIDs[_job]))])\n",
    "\n",
    "        p1 =  torch.LongTensor(pos1)\n",
    "        n1 =  torch.LongTensor(neg1)\n",
    "        sp = model.predict_1(p1).item()\n",
    "        sn = model.predict_1(n1).item()\n",
    "        #sp = model.predict(userIDs[u], itemIDs[i], itemIDs[j])\n",
    "        #sn = model.predict(userIDs[u], itemIDs[k], itemIDs[j])\n",
    "        if sp > sn:\n",
    "            win += 1\n",
    "    return win/N\n",
    "\n",
    "def AUC(model):\n",
    "    av = []\n",
    "    cnt = 0\n",
    "    for u in interactionsTestPerUser:\n",
    "        if cnt % 5000 == 0:\n",
    "            print(cnt)\n",
    "        cnt += 1\n",
    "#         if cnt > 10000:\n",
    "#             break\n",
    "        av.append(AUCu(model, u, 10))\n",
    "    return sum(av) / len(av)\n",
    "AUC(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66204054",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "018b9bbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "AUC is 0.8203200000000108 - FPMC + category + price + education + jobs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07453f20",
   "metadata": {},
   "outputs": [],
   "source": [
    "0.815501519756798"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f6f8084",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def AUCu(model, u, N):\n",
    "#     win = 0\n",
    "#     positive = [random.sample(interactionsTestPerUser[u],1)[0]] * N\n",
    "#     negative = random.sample(itemSet,N)\n",
    "#     _pos =  torch.LongTensor()\n",
    "#     _neg =  torch.LongTensor()\n",
    "#     cnt = 0\n",
    "#     for (i,j,_cat,_year,_price,_hours,_ed,_job, prev_year),k in zip(positive,negative):\n",
    "# #         neg1 = np.array([feat((userIDs[u], itemIDs[k], itemIDs[j], categoryIDs[cat], \\\n",
    "# #                                         yearIDs[_year],priceIDs[_price], hourIDs[_hours], \\\n",
    "# #                                               educationIDs[_ed], jobIDs[_job]   ))])\n",
    "\n",
    "#         neg1 = np.array([feat((userIDs[u], itemIDs[k],itemIDs[j],categoryIDs[_cat] ))])\n",
    "#         neg1 =  torch.LongTensor(neg1)\n",
    "#         if cnt == 0:\n",
    "# #             pos1 = np.array([feat((userIDs[u], itemIDs[j], itemIDs[j], categoryIDs[cat], \\\n",
    "# #                                         yearIDs[_year],priceIDs[_price], hourIDs[_hours], \\\n",
    "# #                                               educationIDs[_ed], jobIDs[_job]   ))])\n",
    "#             pos1 = np.array([feat((userIDs[u], itemIDs[i], itemIDs[j], categoryIDs[_cat]))])\n",
    "#             pos1 =  torch.LongTensor(pos1)\n",
    "#             _pos = pos1\n",
    "#             _neg = neg1\n",
    "#             cnt += 1\n",
    "#             continue\n",
    "#         _neg = torch.cat((_neg, neg1))\n",
    "#     sp = model.predict_1(_pos).item()\n",
    "#     n1 = model.predict_2(_neg).detach().numpy()\n",
    "#     win = sum([int (sp > sn) for sn in n1 ])\n",
    "#     return win/N"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
