{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e862fc52",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import torch as torch\n",
    "import torch.utils.data as data\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import time\n",
    "import numpy as np\n",
    "from sklearn.manifold import TSNE\n",
    "\n",
    "import math\n",
    "from torch.utils.data import Dataset\n",
    "import itertools\n",
    "import torch.nn.functional as F\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "import gzip\n",
    "import numpy as np\n",
    "import random\n",
    "import scipy\n",
    "from collections import defaultdict\n",
    "from scipy.spatial import distance\n",
    "import dateutil\n",
    "import os\n",
    "from sklearn.metrics import roc_auc_score\n",
    "import json, time\n",
    "import pandas as pd\n",
    "def parseData(fname):\n",
    "    for l in open(fname):\n",
    "        yield eval(l)\n",
    "        \n",
    "def readDataFull(path):\n",
    "    data = []\n",
    "    for line in gzip.open(path):\n",
    "        d = eval(line)\n",
    "        data.append(d)  \n",
    "    return data\n",
    "\n",
    "def readData_full(path):\n",
    "    data = []\n",
    "    for line in open(path):\n",
    "        d = json.loads(line)\n",
    "        data.append(d)  \n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9f0daaa3",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataFolder = 'data/ca_final/'\n",
    "users_file_name = dataFolder + 'ca_users.json'\n",
    "places_file_name = dataFolder + 'ca_places.json'\n",
    "reviews_file_name = dataFolder + 'ca_final_reviews.json'\n",
    "\n",
    "data_user  = pd.DataFrame(readData_full(users_file_name))\n",
    "data_places  = pd.DataFrame(readData_full(places_file_name))\n",
    "data_reviews  = pd.DataFrame(readData_full(reviews_file_name))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f2943e7",
   "metadata": {},
   "source": [
    "### Construct data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "27f2cef8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "places_price_dict = {}\n",
    "places_hours_dict = {}\n",
    "\n",
    "user_education = {}\n",
    "user_jobs = {}\n",
    "review_years = {}\n",
    "\n",
    "cnt = 0\n",
    "\n",
    "_uni = ['university','college', 'school','institute', 'institution','academy','polytechnic','varsity', 'academy']\n",
    "def checkeducation(ed):\n",
    "    ed = ed.lower()\n",
    "    for x in _uni:\n",
    "        if x in ed:\n",
    "            return 1\n",
    "    return 0\n",
    "\n",
    "for _i, d in data_user.iterrows():\n",
    "    ed = str(d['education'])\n",
    "    user_education[d['gPlusUserId']] = checkeducation(ed)\n",
    "    if d['jobs'] is not None:\n",
    "        user_jobs[d['gPlusUserId']] = 1\n",
    "    else:\n",
    "        user_jobs[d['gPlusUserId']] = 0\n",
    "    \n",
    "for _i, d in data_places.iterrows():\n",
    "    places_price_dict[d['gPlusPlaceId']] = str(d['price'])\n",
    "    s = str(d['hours'])\n",
    "    s = re.sub(r'[^\\w\\s]','',s)\n",
    "    s = ' '.join( [w for w in s.split() if len(w)>1] )\n",
    "    places_hours_dict[d['gPlusPlaceId']] = s\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f6dfc747",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "17.33063829787234\n"
     ]
    }
   ],
   "source": [
    "userIDs = {}\n",
    "itemIDs = {}\n",
    "categoryIDs = {}\n",
    "yearIDs = {}\n",
    "priceIDs = {}\n",
    "hourIDs = {}\n",
    "educationIDs = {}\n",
    "jobIDs = {}\n",
    "\n",
    "\n",
    "interactions = []\n",
    "interactionsPerUser = defaultdict(list)\n",
    "userVisitedPlaces = defaultdict(set)\n",
    "uniquePlaces = set()\n",
    "\n",
    "\n",
    "cnt = 0\n",
    "for _i, d in data_reviews.iterrows():\n",
    "    u = d['gPlusUserId']\n",
    "    i = d['gPlusPlaceId']\n",
    "    t = d['unixReviewTime']\n",
    "    r = d['rating']\n",
    "    \n",
    "    _cat = str(d['categories'])\n",
    "    _year = 'dummy'\n",
    "    if d['reviewTime'] is not None:\n",
    "        _year = dateutil.parser.parse(d['reviewTime']).year\n",
    "    \n",
    "    _price = places_price_dict[i]\n",
    "    _hours = places_hours_dict[i]\n",
    "    _ed = 'dummy'\n",
    "    if u in user_education:\n",
    "        _ed = user_education[u]\n",
    "    _job = 'dummy'\n",
    "    if u in user_jobs:\n",
    "        _job = user_jobs[u]\n",
    "    \n",
    "    uniquePlaces.add(i)\n",
    "    \n",
    "    if not u in userIDs: userIDs[u] = len(userIDs)\n",
    "    if not i in itemIDs: itemIDs[i] = len(itemIDs)\n",
    "        \n",
    "    if not _cat in categoryIDs: categoryIDs[_cat] = len(categoryIDs)\n",
    "    if not _year in yearIDs: yearIDs[_year] = len(yearIDs)\n",
    "    if not _price in priceIDs: priceIDs[_price] = len(priceIDs)\n",
    "    if not _hours in hourIDs: hourIDs[_hours] = len(hourIDs)\n",
    "    if not _ed in educationIDs: educationIDs[_ed] = len(educationIDs)\n",
    "    if not _job in jobIDs: jobIDs[_job] = len(jobIDs)\n",
    "        \n",
    "    interactions.append((t,u,i,r, _cat,_year,_price,_hours,_ed,_job))\n",
    "    interactionsPerUser[u].append((t,i,r,_cat,_year,_price,_hours,_ed,_job))\n",
    "    userVisitedPlaces[u].add(i)\n",
    "    \n",
    "interactions.sort()\n",
    "userInteractionAvg = sum ([len(interactionsPerUser[u]) for u in interactionsPerUser])  / len(interactionsPerUser)\n",
    "print(userInteractionAvg)\n",
    "\n",
    "itemIDs['dummy'] = len(itemIDs)\n",
    "educationIDs['dummy'] = len(educationIDs)\n",
    "jobIDs['dummy'] = len(jobIDs)\n",
    "yearIDs['dummy'] = len(yearIDs)\n",
    "\n",
    "interactionstrain = []\n",
    "interactionstest = []\n",
    "for u in interactionsPerUser:\n",
    "    interactionsPerUser[u].sort()\n",
    "    list_users = interactionsPerUser[u]\n",
    "    lastItem = 'dummy'\n",
    "    prev_year = 'dummy'\n",
    "    for (t,i,r, _cat,_year,_price,_hours,_ed,_job) in list_users[:-1]:\n",
    "        interactionstrain.append((u,i,lastItem, _cat,_year,_price,_hours,_ed,_job, prev_year))\n",
    "        lastItem = i\n",
    "        prev_year = _year\n",
    "\n",
    "    (t,i,r, _cat,_year,_price,_hours,_ed,_job) = list_users[-1]\n",
    "    interactionstest.append((u,i,lastItem,_cat,_year,_price,_hours,_ed,_job, prev_year))\n",
    "    lastItem = i\n",
    "    prev_year = _year\n",
    "\n",
    "itemsPerUser = defaultdict(set)\n",
    "for (u,i,lastItem,_cat,_year,_price,_hours,_ed,_job, prev_year) in interactionstrain:\n",
    "    itemsPerUser[u].add(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "cede231c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16450 62003 15254 18 5 17605 3 3\n",
      "268639\n",
      "16450\n"
     ]
    }
   ],
   "source": [
    "nUsers,nItems,nCat,nYear, nPrice, nHour, nED, nJob = len(userIDs),len(itemIDs),len(categoryIDs), \\\n",
    "                                                        len(yearIDs), len(priceIDs), len(hourIDs), \\\n",
    "                                                            len(educationIDs), len(jobIDs)\n",
    "items = list(itemIDs.keys())\n",
    "print(nUsers,nItems,nCat,nYear, nPrice, nHour, nED, nJob)\n",
    "print(len(interactionstrain))\n",
    "print(len(interactionstest))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "feb9aa1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#features_list = ['userId_index', 'placeId_index', 'lastplaceId_index', 'cat']\n",
    "features_sizes = {\n",
    "    'userId_index': nUsers,\n",
    "    'placeId_index':nItems,\n",
    "    'lastplaceId_index':nItems,\n",
    "     #'cat':nCat\n",
    "     'year': nYear,\n",
    "     'prev_year': nYear\n",
    "#     'price': nPrice,\n",
    "#     'hour': nHour,\n",
    "#     'ed': nED,\n",
    "#     'job': nJob\n",
    "    \n",
    "}\n",
    "\n",
    "next_offset = 0\n",
    "features_offsets={}\n",
    "index = 0\n",
    "for k,v in features_sizes.items():\n",
    "    features_offsets[index] = next_offset\n",
    "    index += 1\n",
    "    next_offset += v\n",
    "features_offsets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f9dd6ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = []\n",
    "x_train_neg = []\n",
    "\n",
    "def feat(x):\n",
    "    return [x[i] + features_offsets[i] for i in range(len(x))]\n",
    "\n",
    "for (u,i,j,_cat,_year,_price,_hours,_ed,_job, prev_year) in interactionstrain:\n",
    "    uindex = userIDs[u]\n",
    "    iindex = itemIDs[i]\n",
    "    jindex = itemIDs[j]\n",
    "    cat_index = categoryIDs[_cat]\n",
    "    year_index = yearIDs[_year]\n",
    "    price_index = priceIDs[_price]\n",
    "    hour_index = hourIDs[_hours]\n",
    "    ed_index = educationIDs[_ed]\n",
    "    job_index = jobIDs[_job]\n",
    "    prev_year_index = yearIDs[prev_year]\n",
    "    \n",
    "    #x_train.append(feat((uindex, iindex, jindex, cat_index,year_index,price_index,hour_index,ed_index,job_index)))\n",
    "    #x_train.append(feat((uindex, iindex, jindex,cat_index)))\n",
    "    \n",
    "    x_train.append(feat((uindex, iindex, jindex,year_index, prev_year_index)))\n",
    "    \n",
    "    k = random.choice(items) # negative sample\n",
    "    while k in itemsPerUser[u]:\n",
    "        k = random.choice(items)\n",
    "    uindex = userIDs[u]\n",
    "    kindex = itemIDs[k]\n",
    "    jindex = itemIDs[j] \n",
    "    cat_index = categoryIDs[_cat]\n",
    "    year_index = yearIDs[_year]\n",
    "    price_index = priceIDs[_price]\n",
    "    hour_index = hourIDs[_hours]\n",
    "    ed_index = educationIDs[_ed]\n",
    "    job_index = jobIDs[_job]\n",
    "    prev_year_index = yearIDs[prev_year]\n",
    "    \n",
    "    x_train_neg.append(feat((uindex, kindex, jindex, year_index, prev_year_index)))\n",
    "    \n",
    "    #x_train_neg.append(feat((uindex, kindex, jindex, cat_index)))\n",
    "    #x_train_neg.append(feat((uindex, kindex, jindex, cat_index,year_index,price_index,hour_index,ed_index,job_index)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2594763e",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_x = torch.tensor(x_train)\n",
    "data_x_neg = torch.tensor(x_train_neg)\n",
    "dataset = data.TensorDataset(data_x,data_x_neg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "677b7884",
   "metadata": {},
   "outputs": [],
   "source": [
    "bs=100000\n",
    "train_n = int(len(dataset)*0.8)\n",
    "valid_n = len(dataset) - train_n\n",
    "splits = [train_n,valid_n]\n",
    "assert sum(splits) == len(dataset)\n",
    "trainset,devset = torch.utils.data.random_split(dataset,splits)\n",
    "train_dataloader = data.DataLoader(trainset,batch_size=bs,shuffle=True)\n",
    "dev_dataloader = data.DataLoader(devset,batch_size=bs,shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53dd3390",
   "metadata": {},
   "outputs": [],
   "source": [
    "def trunc_normal_(x, mean=0., std=1.):\n",
    "    \"Truncated normal initialization.\"\n",
    "    return x.normal_().fmod_(2).mul_(std).add_(mean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2f7d989",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FMModel(nn.Module):\n",
    "    def __init__(self, n, k):\n",
    "        super().__init__()\n",
    "\n",
    "        self.w0 = nn.Parameter(torch.zeros(1))\n",
    "        self.bias = nn.Embedding(n, 1)\n",
    "        self.embeddings = nn.Embedding(n, k)\n",
    "\n",
    "        with torch.no_grad(): trunc_normal_(self.embeddings.weight, std=0.01)\n",
    "        with torch.no_grad(): trunc_normal_(self.bias.weight, std=0.01)\n",
    "\n",
    "    def forward(self, X_pos, X_neg):\n",
    "        emb = self.embeddings(X_pos)\n",
    "        pow_of_sum = emb.sum(dim=1).pow(2)\n",
    "        sum_of_pow = emb.pow(2).sum(dim=1)\n",
    "        pairwise = (pow_of_sum-sum_of_pow).sum(1)*0.5\n",
    "        bias = self.bias(X_pos).squeeze().sum(1)\n",
    "        \n",
    "        pos = self.w0 + bias + pairwise\n",
    "        \n",
    "        emb = self.embeddings(X_neg)\n",
    "        pow_of_sum = emb.sum(dim=1).pow(2)\n",
    "        sum_of_pow = emb.pow(2).sum(dim=1)\n",
    "        pairwise = (pow_of_sum-sum_of_pow).sum(1)*0.5\n",
    "        bias = self.bias(X_neg).squeeze().sum(1)        \n",
    "        \n",
    "        neg = self.w0 + bias + pairwise\n",
    "        loss = -torch.mean(torch.log(torch.sigmoid(pos - neg)))\n",
    "        return loss\n",
    "    \n",
    "    def predict_1(self, X):\n",
    "        \n",
    "        emb = self.embeddings(X)\n",
    "        pow_of_sum = emb.sum(dim=1).pow(2)\n",
    "        sum_of_pow = emb.pow(2).sum(dim=1)\n",
    "        pairwise = (pow_of_sum-sum_of_pow).sum(1)*0.5\n",
    "        bias = self.bias(X).sum(1)\n",
    "        \n",
    "        return self.w0 + bias + pairwise \n",
    "    def predict_2(self, X):\n",
    "        \n",
    "        emb = self.embeddings(X)\n",
    "        pow_of_sum = emb.sum(dim=1).pow(2)\n",
    "        sum_of_pow = emb.pow(2).sum(dim=1)\n",
    "        pairwise = (pow_of_sum-sum_of_pow).sum(1)*0.5\n",
    "        bias = self.bias(X).squeeze().sum(1) \n",
    "        \n",
    "        return self.w0 + bias + pairwise \n",
    "        #return pos - neg\n",
    "    \n",
    "        #return torch.mean(torch.log(torch.sigmoid(pos - neg)))\n",
    "    \n",
    "        #return -tf.reduce_mean(tf.math.log(tf.math.sigmoid(x_uij - x_ukj)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7561c1b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit(iterator, model, optimizer, criterion):\n",
    "    train_loss = 0\n",
    "    model.train()\n",
    "    for x_pos,x_neg in iterator:\n",
    "        optimizer.zero_grad()\n",
    "        loss = model(x_pos, x_neg)\n",
    "        train_loss += loss.item()*x_pos.shape[0]\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    return train_loss / len(iterator.dataset)\n",
    "\n",
    "def test(iterator, model, criterion):\n",
    "    train_loss = 0\n",
    "    model.eval()\n",
    "    for x_pos,x_neg in iterator:                    \n",
    "        with torch.no_grad():\n",
    "            loss = model(x_pos, x_neg)\n",
    "        train_loss += loss.item()*x_pos.shape[0]\n",
    "    return train_loss / len(iterator.dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2813fd17",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#model = FMModel(max(data_x_neg.max(), data_x.max())+1, 5)\n",
    "model = FMModel(data_x.max()+1, 5)\n",
    "wd=1e-5\n",
    "lr=0.05\n",
    "epochs=15\n",
    "optimizer = optim.Adam(model.parameters(), lr=lr, weight_decay=wd)\n",
    "scheduler = optim.lr_scheduler.MultiStepLR(optimizer, milestones=[10], gamma=0.1)\n",
    "criterion = nn.MSELoss()\n",
    "for epoch in range(epochs):\n",
    "    start_time = time.time()\n",
    "    train_loss = fit(train_dataloader, model, optimizer, criterion)\n",
    "    valid_loss = test(dev_dataloader, model, criterion)\n",
    "    scheduler.step()\n",
    "    secs = int(time.time() - start_time)\n",
    "    print(f'epoch {epoch}. time: {secs}[s]')\n",
    "    print(f'\\ttrain loss: {((train_loss)):.4f}')\n",
    "    print(f'\\tvalidation loss: {((valid_loss)):.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36a462b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "interactionsTestPerUser = defaultdict(set)\n",
    "itemSet = set()\n",
    "for (u,i,j,_cat,_year,_price,_hours,_ed,_job) in interactionstest:\n",
    "    interactionsTestPerUser[u].add((i,j,_cat,_year,_price,_hours,_ed,_job))\n",
    "    itemSet.add(i)\n",
    "    itemSet.add(j)\n",
    "    \n",
    "def AUCu(model, u, N):\n",
    "    win = 0\n",
    "    positive = [random.sample(interactionsTestPerUser[u],1)[0]] * N\n",
    "    negative = random.sample(itemSet,N)\n",
    "    _pos =  torch.LongTensor()\n",
    "    _neg =  torch.LongTensor()\n",
    "    cnt = 0\n",
    "    for (i,j,_cat,_year,_price,_hours,_ed,_job),k in zip(positive,negative):\n",
    "#         neg1 = np.array([feat((userIDs[u], itemIDs[k], itemIDs[j], categoryIDs[cat], \\\n",
    "#                                         yearIDs[_year],priceIDs[_price], hourIDs[_hours], \\\n",
    "#                                               educationIDs[_ed], jobIDs[_job]   ))])\n",
    "\n",
    "        neg1 = np.array([feat((userIDs[u], itemIDs[k],itemIDs[j],categoryIDs[_cat] ))])\n",
    "        neg1 =  torch.LongTensor(neg1)\n",
    "        if cnt == 0:\n",
    "#             pos1 = np.array([feat((userIDs[u], itemIDs[j], itemIDs[j], categoryIDs[cat], \\\n",
    "#                                         yearIDs[_year],priceIDs[_price], hourIDs[_hours], \\\n",
    "#                                               educationIDs[_ed], jobIDs[_job]   ))])\n",
    "            pos1 = np.array([feat((userIDs[u], itemIDs[i], itemIDs[j], categoryIDs[_cat]))])\n",
    "            pos1 =  torch.LongTensor(pos1)\n",
    "            _pos = pos1\n",
    "            _neg = neg1\n",
    "            cnt += 1\n",
    "            continue\n",
    "        _neg = torch.cat((_neg, neg1))\n",
    "    sp = model.predict_1(_pos).item()\n",
    "    n1 = model.predict_2(_neg).detach().numpy()\n",
    "    win = sum([int (sp > sn) for sn in n1 ])\n",
    "    return win/N\n",
    "\n",
    "# def AUCu(model, u, N):\n",
    "#     win = 0\n",
    "#     positive = [random.sample(interactionsTestPerUser[u],1)[0]] * N\n",
    "#     negative = random.sample(itemSet,N)\n",
    "#     for (i,j,cat,_year,_price,_hours,_ed,_job),k  in zip(positive,negative):\n",
    "#         pos1 = np.array([feat((userIDs[u], itemIDs[i], itemIDs[j], categoryIDs[cat]))])\n",
    "#         neg1 = np.array([feat((userIDs[u], itemIDs[k], itemIDs[j], categoryIDs[cat] ))])\n",
    "#         p1 =  torch.LongTensor(pos1)\n",
    "#         n1 =  torch.LongTensor(neg1)\n",
    "#         sp = model.predict_1(p1).item()\n",
    "#         sn = model.predict_1(n1).item()\n",
    "#         #sp = model.predict(userIDs[u], itemIDs[i], itemIDs[j])\n",
    "#         #sn = model.predict(userIDs[u], itemIDs[k], itemIDs[j])\n",
    "#         if sp > sn:\n",
    "#             win += 1\n",
    "#     return win/N\n",
    "    \n",
    "\n",
    "def AUC(model):\n",
    "    av = []\n",
    "    cnt = 0\n",
    "    for u in interactionsTestPerUser:\n",
    "        if cnt % 5000 == 0:\n",
    "            print(cnt)\n",
    "        cnt += 1\n",
    "#         if cnt > 10000:\n",
    "#             break\n",
    "        av.append(AUCu(model, u, 10))\n",
    "    return sum(av) / len(av)\n",
    "AUC(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e7c690b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a757f9a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06b4b5f3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
