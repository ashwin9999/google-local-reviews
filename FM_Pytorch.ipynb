{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 337,
   "id": "a45a6feb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import torch as torch\n",
    "import torch.utils.data as data\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import time\n",
    "import numpy as np\n",
    "from sklearn.manifold import TSNE\n",
    "\n",
    "import math\n",
    "from torch.utils.data import Dataset\n",
    "import itertools\n",
    "import torch.nn.functional as F\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "import gzip\n",
    "import numpy as np\n",
    "import random\n",
    "import scipy\n",
    "from collections import defaultdict\n",
    "from scipy.spatial import distance\n",
    "import dateutil\n",
    "import os\n",
    "from sklearn.metrics import roc_auc_score\n",
    "import json, time\n",
    "import pandas as pd\n",
    "def parseData(fname):\n",
    "    for l in open(fname):\n",
    "        yield eval(l)\n",
    "        \n",
    "def readDataFull(path):\n",
    "    data = []\n",
    "    for line in gzip.open(path):\n",
    "        d = eval(line)\n",
    "        data.append(d)  \n",
    "    return data\n",
    "\n",
    "def readData_full(path):\n",
    "    data = []\n",
    "    for line in open(path):\n",
    "        d = json.loads(line)\n",
    "        data.append(d)  \n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 341,
   "id": "fd9be324",
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataFolder = 'data/filtered_data/'\n",
    "# users_file_name = dataFolder + 'filtered_users.json'\n",
    "# places_file_name = dataFolder + 'filtered_places.json'\n",
    "# reviews_file_name = dataFolder + 'filtered_reviews.json'\n",
    "\n",
    "# data_user  = pd.DataFrame(json.load(open(users_file_name)))\n",
    "# data_places = pd.DataFrame(json.load(open(places_file_name)))\n",
    "# data_reviews  = pd.DataFrame(json.load(open(reviews_file_name)))\n",
    "\n",
    "dataFolder = 'data/ca_final/'\n",
    "users_file_name = dataFolder + 'ca_users.json'\n",
    "places_file_name = dataFolder + 'ca_places.json'\n",
    "reviews_file_name = dataFolder + 'ca_final_reviews.json'\n",
    "\n",
    "data_user  = pd.DataFrame(readData_full(users_file_name))\n",
    "data_places  = pd.DataFrame(readData_full(places_file_name))\n",
    "data_reviews  = pd.DataFrame(readData_full(reviews_file_name))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d9781b9",
   "metadata": {},
   "source": [
    "### Construct data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 342,
   "id": "a4b2969a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "17.33063829787234\n"
     ]
    }
   ],
   "source": [
    "userIDs = {}\n",
    "itemIDs = {}\n",
    "interactions = []\n",
    "interactionsPerUser = defaultdict(list)\n",
    "userVisitedPlaces = defaultdict(set)\n",
    "uniquePlaces = set()\n",
    "for _i, d in data_reviews.iterrows():\n",
    "    u = d['gPlusUserId']\n",
    "    i = d['gPlusPlaceId']\n",
    "    t = d['unixReviewTime']\n",
    "    r = d['rating']\n",
    "    uniquePlaces.add(i)\n",
    "    #dt = dateutil.parser.parse(t)\n",
    "    #t = int(dt.timestamp())\n",
    "    if not u in userIDs: userIDs[u] = len(userIDs)\n",
    "    if not i in itemIDs: itemIDs[i] = len(itemIDs)\n",
    "    interactions.append((t,u,i,r))\n",
    "    interactionsPerUser[u].append((t,i,r))\n",
    "    userVisitedPlaces[u].add(i)\n",
    "    \n",
    "interactions.sort()\n",
    "userInteractionAvg = sum ([len(interactionsPerUser[u]) for u in interactionsPerUser])  / len(interactionsPerUser)\n",
    "print(userInteractionAvg)\n",
    "\n",
    "itemIDs['dummy'] = len(itemIDs)\n",
    "\n",
    "\n",
    "interactionstrain = []\n",
    "interactionstest = []\n",
    "for u in interactionsPerUser:\n",
    "    interactionsPerUser[u].sort()\n",
    "    list_users = interactionsPerUser[u]\n",
    "    lastItem = 'dummy'\n",
    "    \n",
    "    for (t,i,r) in list_users[:-1]:\n",
    "        interactionstrain.append((u,i,lastItem))\n",
    "        lastItem = i\n",
    "\n",
    "    (t,i,r) = list_users[-1]\n",
    "    interactionstest.append((u,i,lastItem))\n",
    "    lastItem = i\n",
    "\n",
    "itemsPerUser = defaultdict(set)\n",
    "for u,i,j in interactionstrain:\n",
    "    itemsPerUser[u].add(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 343,
   "id": "bef1d8da",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16450 62003\n",
      "268639\n",
      "16450\n"
     ]
    }
   ],
   "source": [
    "nUsers,nItems = len(userIDs),len(itemIDs)\n",
    "items = list(itemIDs.keys())\n",
    "print(nUsers,nItems)\n",
    "print(len(interactionstrain))\n",
    "print(len(interactionstest))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 344,
   "id": "7685e1fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "features_list = ['userId_index', 'placeId_index', 'lastplaceId_index']\n",
    "features_sizes = {\n",
    "    'userId_index': nUsers,\n",
    "    'placeId_index':nItems,\n",
    "    'lastplaceId_index':nItems\n",
    "#     'age_index':len(ratings['age_index'].unique()),\n",
    "#     'gender_index':len(ratings['gender_index'].unique()),\n",
    "#     'occupation_index':len(ratings['occupation_index'].unique()),\n",
    "}\n",
    "\n",
    "next_offset = 0\n",
    "features_offsets={}\n",
    "index = 0\n",
    "for k,v in features_sizes.items():\n",
    "    features_offsets[index] = next_offset\n",
    "    index += 1\n",
    "    next_offset += v"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 345,
   "id": "ce528ca0",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = []\n",
    "x_train_neg = []\n",
    "\n",
    "def feat(x):\n",
    "    return [x[i] + features_offsets[i] for i in range(len(x))]\n",
    "\n",
    "for (u,i,j) in interactionstrain:\n",
    "    uindex = userIDs[u]\n",
    "    iindex = itemIDs[i]\n",
    "    jindex = itemIDs[j]\n",
    "    x_train.append(feat((uindex, iindex, jindex)))\n",
    "    \n",
    "    k = random.choice(items) # negative sample\n",
    "    while k in itemsPerUser[u]:\n",
    "        k = random.choice(items)\n",
    "    uindex = userIDs[u]\n",
    "    kindex = itemIDs[k]\n",
    "    jindex = itemIDs[j]   \n",
    "    x_train_neg.append(feat((uindex, kindex, jindex)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 346,
   "id": "926a55b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_x = torch.tensor(x_train)\n",
    "data_x_neg = torch.tensor(x_train_neg)\n",
    "dataset = data.TensorDataset(data_x,data_x_neg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 363,
   "id": "4899e375",
   "metadata": {},
   "outputs": [],
   "source": [
    "bs=50000\n",
    "train_n = int(len(dataset)*0.8)\n",
    "valid_n = len(dataset) - train_n\n",
    "splits = [train_n,valid_n]\n",
    "assert sum(splits) == len(dataset)\n",
    "trainset,devset = torch.utils.data.random_split(dataset,splits)\n",
    "train_dataloader = data.DataLoader(trainset,batch_size=bs,shuffle=True)\n",
    "dev_dataloader = data.DataLoader(devset,batch_size=bs,shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 364,
   "id": "900c45d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# copied from fastai: \n",
    "def trunc_normal_(x, mean=0., std=1.):\n",
    "    \"Truncated normal initialization.\"\n",
    "    # From https://discuss.pytorch.org/t/implementing-truncated-normal-initializer/4778/12\n",
    "    return x.normal_().fmod_(2).mul_(std).add_(mean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 365,
   "id": "a89c4b09",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FMModel(nn.Module):\n",
    "    def __init__(self, n, k):\n",
    "        super().__init__()\n",
    "\n",
    "        self.w0 = nn.Parameter(torch.zeros(1))\n",
    "        self.bias = nn.Embedding(n, 1)\n",
    "        self.embeddings = nn.Embedding(n, k)\n",
    "\n",
    "        with torch.no_grad(): trunc_normal_(self.embeddings.weight, std=0.01)\n",
    "        with torch.no_grad(): trunc_normal_(self.bias.weight, std=0.01)\n",
    "\n",
    "    def forward(self, X_pos, X_neg):\n",
    "        emb = self.embeddings(X_pos)\n",
    "        pow_of_sum = emb.sum(dim=1).pow(2)\n",
    "        sum_of_pow = emb.pow(2).sum(dim=1)\n",
    "        pairwise = (pow_of_sum-sum_of_pow).sum(1)*0.5\n",
    "        bias = self.bias(X_pos).squeeze().sum(1)\n",
    "        \n",
    "        pos = self.w0 + bias + pairwise\n",
    "        \n",
    "        emb = self.embeddings(X_neg)\n",
    "        pow_of_sum = emb.sum(dim=1).pow(2)\n",
    "        sum_of_pow = emb.pow(2).sum(dim=1)\n",
    "        pairwise = (pow_of_sum-sum_of_pow).sum(1)*0.5\n",
    "        bias = self.bias(X_neg).squeeze().sum(1)        \n",
    "        \n",
    "        neg = self.w0 + bias + pairwise\n",
    "        loss = -torch.mean(torch.log(torch.sigmoid(pos - neg)))\n",
    "        return loss\n",
    "    \n",
    "    def predict_1(self, X):\n",
    "        \n",
    "        emb = self.embeddings(X)\n",
    "        pow_of_sum = emb.sum(dim=1).pow(2)\n",
    "        sum_of_pow = emb.pow(2).sum(dim=1)\n",
    "        pairwise = (pow_of_sum-sum_of_pow).sum(1)*0.5\n",
    "        bias = self.bias(X).sum(1)\n",
    "        \n",
    "        return self.w0 + bias + pairwise \n",
    "    def predict_2(self, X):\n",
    "        \n",
    "        emb = self.embeddings(X)\n",
    "        pow_of_sum = emb.sum(dim=1).pow(2)\n",
    "        sum_of_pow = emb.pow(2).sum(dim=1)\n",
    "        pairwise = (pow_of_sum-sum_of_pow).sum(1)*0.5\n",
    "        bias = self.bias(X).squeeze().sum(1) \n",
    "        \n",
    "        return self.w0 + bias + pairwise \n",
    "        #return pos - neg\n",
    "    \n",
    "        #return torch.mean(torch.log(torch.sigmoid(pos - neg)))\n",
    "    \n",
    "        #return -tf.reduce_mean(tf.math.log(tf.math.sigmoid(x_uij - x_ukj)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 366,
   "id": "a57d2682",
   "metadata": {},
   "outputs": [],
   "source": [
    "# fit/test functions\n",
    "def fit(iterator, model, optimizer, criterion):\n",
    "    train_loss = 0\n",
    "    model.train()\n",
    "    for x_pos,x_neg in iterator:\n",
    "        optimizer.zero_grad()\n",
    "        loss = model(x_pos, x_neg)\n",
    "        train_loss += loss.item()*x_pos.shape[0]\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    return train_loss / len(iterator.dataset)\n",
    "\n",
    "def test(iterator, model, criterion):\n",
    "    train_loss = 0\n",
    "    model.eval()\n",
    "    for x_pos,x_neg in iterator:                    \n",
    "        with torch.no_grad():\n",
    "            loss = model(x_pos, x_neg)\n",
    "        train_loss += loss.item()*x_pos.shape[0]\n",
    "    return train_loss / len(iterator.dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 371,
   "id": "42fa1b56",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0. time: 6[s]\n",
      "\ttrain loss: 0.6583\n",
      "\tvalidation loss: 0.5939\n",
      "epoch 1. time: 5[s]\n",
      "\ttrain loss: 0.4992\n",
      "\tvalidation loss: 0.5120\n",
      "epoch 2. time: 1[s]\n",
      "\ttrain loss: 0.3725\n",
      "\tvalidation loss: 0.4674\n",
      "epoch 3. time: 6[s]\n",
      "\ttrain loss: 0.2907\n",
      "\tvalidation loss: 0.4516\n",
      "epoch 4. time: 6[s]\n",
      "\ttrain loss: 0.2460\n",
      "\tvalidation loss: 0.4488\n",
      "epoch 5. time: 1[s]\n",
      "\ttrain loss: 0.2306\n",
      "\tvalidation loss: 0.4494\n",
      "epoch 6. time: 6[s]\n",
      "\ttrain loss: 0.2283\n",
      "\tvalidation loss: 0.4472\n",
      "epoch 7. time: 6[s]\n",
      "\ttrain loss: 0.2270\n",
      "\tvalidation loss: 0.4438\n",
      "epoch 8. time: 5[s]\n",
      "\ttrain loss: 0.2244\n",
      "\tvalidation loss: 0.4433\n",
      "epoch 9. time: 1[s]\n",
      "\ttrain loss: 0.2230\n",
      "\tvalidation loss: 0.4442\n"
     ]
    }
   ],
   "source": [
    "model = FMModel(data_x.max()+1, 5)\n",
    "wd=1e-5\n",
    "lr=0.1\n",
    "epochs=10\n",
    "optimizer = optim.Adam(model.parameters(), lr=lr, weight_decay=wd)\n",
    "scheduler = optim.lr_scheduler.MultiStepLR(optimizer, milestones=[20], gamma=0.1)\n",
    "criterion = nn.MSELoss()\n",
    "for epoch in range(epochs):\n",
    "    start_time = time.time()\n",
    "    train_loss = fit(train_dataloader, model, optimizer, criterion)\n",
    "    valid_loss = test(dev_dataloader, model, criterion)\n",
    "    scheduler.step()\n",
    "    secs = int(time.time() - start_time)\n",
    "    print(f'epoch {epoch}. time: {secs}[s]')\n",
    "    print(f'\\ttrain loss: {((train_loss)):.4f}')\n",
    "    print(f'\\tvalidation loss: {((valid_loss)):.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 372,
   "id": "783d4282",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "5000\n",
      "10000\n",
      "15000\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.627057750759867"
      ]
     },
     "execution_count": 372,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "interactionsTestPerUser = defaultdict(set)\n",
    "itemSet = set()\n",
    "for u,i,j in interactionstest:\n",
    "    interactionsTestPerUser[u].add((i,j))\n",
    "    itemSet.add(i)\n",
    "    itemSet.add(j)\n",
    "    \n",
    "def AUCu(model, u, N):\n",
    "    win = 0\n",
    "    positive = [random.sample(interactionsTestPerUser[u],1)[0]] * N\n",
    "    negative = random.sample(itemSet,N)\n",
    "    for (i,j),k in zip(positive,negative):\n",
    "        pos1 = np.array([feat((userIDs[u], itemIDs[i], itemIDs[j]))])\n",
    "        neg1 = np.array([feat((userIDs[u], itemIDs[k], itemIDs[j]))])\n",
    "        p1 =  torch.LongTensor(pos1)\n",
    "        n1 =  torch.LongTensor(neg1)\n",
    "        sp = model.predict_1(p1).item()\n",
    "        sn = model.predict_1(n1).item()\n",
    "        #sp = model.predict(userIDs[u], itemIDs[i], itemIDs[j])\n",
    "        #sn = model.predict(userIDs[u], itemIDs[k], itemIDs[j])\n",
    "        if sp > sn:\n",
    "            win += 1\n",
    "    return win/N\n",
    "\n",
    "def AUC(model):\n",
    "    av = []\n",
    "    cnt = 0\n",
    "    for u in interactionsTestPerUser:\n",
    "        if cnt % 5000 == 0:\n",
    "            print(cnt)\n",
    "        cnt += 1\n",
    "        av.append(AUCu(model, u, 10))\n",
    "    return sum(av) / len(av)\n",
    "AUC(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9403fd9f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9163bfa",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
